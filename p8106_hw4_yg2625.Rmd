---
title: "p8106_hw4_yg2625"
author: "Yue Gu"
date: "April 21, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lasso2) # only for data
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(tidyverse)
set.seed(1)
```

**1. This problem involves the Prostate data in the lasso2 package (see L5.Rmd). Use set.seed() for reproducible results.**

## load data
```{r}
data("Prostate")
pros_data = Prostate%>% 
  janitor::clean_names()
```


# (a) Fit a regression tree with lpsa as the response and the other variables as predictors. Use cross-validation to determine the optimal tree size. Which tree size corresponds to the lowest cross-validation error? Is this the same as the tree size obtained using the 1 SE rule?


```{r}
# use cross-validation through caret
ctrl <- trainControl(method = "cv")

# tune over cp, method = "rpart"
rpart.fit1 <- train(lpsa ~ ., pros_data, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = ctrl)
ggplot(rpart.fit1, highlight = TRUE)
# cptable showed that the optimal tree size is 8
rpart.fit1$finalModel$cptable
rpart.plot(rpart.fit1$finalModel)


# use 1SE through caret
rpart.fit2 <- train(lpsa ~ ., pros_data, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = trainControl(method = 'cv',
                                            number = 10,
                                            selectionFunction = 'oneSE'))

# cptable showed that the optimal tree size is 3
rpart.fit2$finalModel$cptable
rpart.plot(rpart.fit2$finalModel)
```

Based on the result, cross-validation showed that the optimal tree size is 8 while 1SE obtained optimal tree size as 3. Hence, 1SE rule generates tree with smaller size.

# (b) Create a plot of the final tree you choose. Pick one of the terminal nodes, and interpret the information displayed.

```{r}
resamp = resamples(list(minErr = rpart.fit1, min_1se = rpart.fit2))
summary(resamp)
```
Since two regression generates similar RMSE. Following principle of parsimony, we choose the simpler model with tree using 1SE principle. And the plot is shown below:
```{r}
rpart.plot(rpart.fit2$finalModel)
```
**Interpretation:** The regression tree pruned by 1SE rule has size as 3. When log(cancer volume)(lcavol) is greater than 2.5, the log(prostate specific antigen) is predicted to be 3.8, which contains 22% of the trainning observations.


# (c) Perform bagging and report the variable importance.
```{r}
bagging <- ranger(lpsa ~., pros_data,
                        mtry = 8, splitrule = "variance",
                        min.node.size = 30,
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(bagging), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```
Based on the output, variable importance ranking: lcavol>lweight>svi>pgg45>lcp>gleason>lbph>age.

# (d) Perform random forests and report the variable importance.
```{r}
set.seed(1)
rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "variance",
                       min.node.size = 1:30)

rf.fit <- train(lpsa ~ ., pros_data,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)
# get best tuning parameter alpha = 5
rf.fit$bestTune
# random forests plot
ggplot(rf.fit, highlight = TRUE)
# fit random forest model using best tuning parameter
rf <- ranger(lpsa ~., pros_data,
                        mtry = 5, splitrule = "variance",
                        min.node.size = 30,
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
summary(rf.fit)
```
Based on the output, variable importance ranking: lcavol>lweight>svi>pgg45>lcp>lbph>gleason>age.

# (e) Perform boosting and report the variable importance.
```{r}
set.seed(1)
gbm.grid <- expand.grid(n.trees = c(2000,2500,3000,3500,4000,4500,5000),
                        interaction.depth = 1:5,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)

gbm.fit <- train(lpsa ~ ., pros_data, 
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 method = "gbm",
                 verbose = FALSE)
# boosting plot
ggplot(gbm.fit, highlight = TRUE)
# best tunning parameter alph = 38
gbm.fit$bestTune
# summary output
summary(gbm.fit)
```
Based on the result, variable importance ranking: lcavol>lweight>svi>lcp>pgg45>age>lbph>gleason.

# (f) Which of the above models will you select to predict PSA level? Explain.
```{r}
resamp2 = resamples(list(minErr = rpart.fit1, 
                         min_1se = rpart.fit2,
                         randomForest = rf.fit,
                         boosting = gbm.fit))

summary(resamp2)
ggplot(resamp2, "RMSE") +
  labs(y = 'RMSE')
```
Based on the output, simple linear regression trees method including minimum CV error and 1SE principle methods generates model with larger RMSE than ensemble methods(bagging, random forest, boosting). And bagging being a special case of random forest selecting 8 predictors(mtry = 8), violated principle of parsimony compared to random forest and boosting methods. Boosting generates model with smaller RMSE than randomForest. Hence, boosting model is most preferrable.


