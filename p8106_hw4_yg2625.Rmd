---
title: "p8106_hw4_yg2625"
author: "Yue Gu"
date: "April 21, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lasso2) # only for data
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(lime)
library(tidyverse)
set.seed(1)
```

**1. This problem involves the Prostate data in the lasso2 package (see L5.Rmd). Use set.seed() for reproducible results.**

## load data
```{r}
data("Prostate")
pros_data = Prostate%>% 
  janitor::clean_names()
```


# (a) Fit a regression tree with lpsa as the response and the other variables as predictors. Use cross-validation to determine the optimal tree size. Which tree size corresponds to the lowest cross-validation error? Is this the same as the tree size obtained using the 1 SE rule?


```{r}
# use cross-validation through caret
ctrl <- trainControl(method = "cv")

# tune over cp, method = "rpart"
rpart.fit1 <- train(lpsa ~ ., pros_data, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = ctrl)
ggplot(rpart.fit1, highlight = TRUE)
# cptable showed that the optimal tree size is 8
rpart.fit1$finalModel$cptable
rpart.plot(rpart.fit1$finalModel)


# use 1SE through caret
rpart.fit2 <- train(lpsa ~ ., pros_data, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = trainControl(method = 'cv',
                                            number = 10,
                                            selectionFunction = 'oneSE'))

# cptable showed that the optimal tree size is 3
rpart.fit2$finalModel$cptable
rpart.plot(rpart.fit2$finalModel)
```

Based on the result, cross-validation showed that the optimal tree size is 8 while 1SE obtained optimal tree size as 3. Hence, 1SE rule generates tree with smaller size.

# (b) Create a plot of the final tree you choose. Pick one of the terminal nodes, and interpret the information displayed.

```{r}
resamp = resamples(list(minErr = rpart.fit1, min_1se = rpart.fit2))
summary(resamp)
```
Since two regression generates similar RMSE. Following principle of parsimony, we choose the simpler model with tree using 1SE principle. And the plot is shown below:
```{r}
rpart.plot(rpart.fit2$finalModel)
```
**Interpretation:** The regression tree pruned by 1SE rule has size as 3. When log(cancer volume)(lcavol) is greater than 2.5, the log(prostate specific antigen) is predicted to be 3.8, which contains 22% of the trainning observations.


# (c) Perform bagging and report the variable importance.
```{r}
bagging <- ranger(lpsa ~., pros_data,
                        mtry = 8, splitrule = "variance",
                        min.node.size = 30,
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(bagging), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```
Based on the output, variable importance ranking: lcavol>lweight>svi>pgg45>lcp>gleason>lbph>age.

# (d) Perform random forests and report the variable importance.
```{r}
set.seed(1)
rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "variance",
                       min.node.size = 1:30)

rf.fit <- train(lpsa ~ ., pros_data,
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl)
# get best tuning parameter alpha = 5
rf.fit$bestTune
# random forests plot
ggplot(rf.fit, highlight = TRUE)
# fit random forest model using best tuning parameter
rf <- ranger(lpsa ~., pros_data,
                        mtry = 5, splitrule = "variance",
                        min.node.size = 30,
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```
Based on the output, variable importance ranking: lcavol>lweight>svi>pgg45>lcp>lbph>gleason>age.

# (e) Perform boosting and report the variable importance.
```{r}
set.seed(1)
gbm.grid <- expand.grid(n.trees = c(2000,2500,3000,3500,4000,4500,5000),
                        interaction.depth = 1:5,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)

gbm.fit <- train(lpsa ~ ., pros_data, 
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 method = "gbm",
                 verbose = FALSE)
# boosting plot
ggplot(gbm.fit, highlight = TRUE)
# best tunning parameter alph = 38
gbm.fit$bestTune
# summary output
summary(gbm.fit)
```
Based on the result, variable importance ranking: lcavol>lweight>svi>lcp>pgg45>age>lbph>gleason.

# (f) Which of the above models will you select to predict PSA level? Explain.
```{r}
resamp2 = resamples(list(minErr = rpart.fit1, 
                         min_1se = rpart.fit2,
                         randomForest = rf.fit,
                         boosting = gbm.fit))

summary(resamp2)
bwplot(resamp2, metric = "RMSE")
```
Based on the output, simple regression trees method including minimum CV error and 1SE principle methods generates model with larger RMSE than ensemble methods(bagging, random forest, boosting). And bagging being a special case of random forest selecting 8 predictors(mtry = 8) without tunning, violated principle of parsimony compared to random forest and boosting methods. Boosting generates model with smaller RMSE than randomForest. Hence, boosting model is most preferrable.

**2. This problem involves the OJ data in the ISLR package. The data contains 1070 purchases where the customers either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of customers and products are recorded. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Use set.seed() for reproducible results.**
## load data
```{r}
data(OJ)
oj_data = OJ %>% 
  janitor::clean_names()

# create a training set containing 800 obs
set.seed(1)
rowTrain = createDataPartition(y = oj_data$purchase,
                               p = 799/1070,
                               list = F)

train_data = oj_data[rowTrain, ]
test_data = oj_data[-rowTrain, ]
# check whether there is 800 obs
dim(train_data)
```


# (a) Fit a classification tree to the training set, with Purchase as the response and the other variables as predictors. Use cross-validation to determine the tree size and create a plot of the final tree. Predict the response on the test data. What is the test classification error rate?

```{r}
ctrl2 <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit_oj <- train(purchase ~ ., train_data,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-20,-5, len = 20))),
                   trControl = ctrl2,
                   metric = "ROC")

ggplot(rpart.fit_oj, highlight = TRUE)
# optimal tree size is 17 with smallest CV error
rpart.fit_oj$finalModel$cptable

# plot of tree
rpart.plot(rpart.fit_oj$finalModel)

# predict response on test data
pred = predict(rpart.fit_oj, newdata = test_data,
               type = "raw");pred

# test classification error rate
1 - mean(test_data$purchase == pred)
```
Hence, the optimal tree size is 17 and the test classification error rate is 0.207.

# (b) Perform random forests on the training set and report variable importance. What is the test error rate?

```{r}
rf.grid_oj <- expand.grid(mtry = seq(2,12,2),
                       splitrule = "gini",
                       min.node.size = seq(20,100,5))
set.seed(1) 
rf.fit_oj <- train(purchase ~ ., train_data,
                method = "ranger",
                tuneGrid = rf.grid_oj,
                metric = "ROC",
                importance = "impurity",
                trControl = ctrl2)
# rf plot
ggplot(rf.fit_oj, highlight = TRUE)
# compare variable importance
barplot(sort(ranger::importance(rf.fit_oj$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

# predict on test data
pred2 = predict(rf.fit_oj, newdata = test_data,
               type = "raw");pred
# test error rate
1 - mean(test_data$purchase == pred2)
```
Based on the result, variable importance ranking top 5: 
loyal_ch > price_diff > store_id > list_price_diff > week_of_purchase, and store7Yes has least importance. The test error rate is 0.185.


